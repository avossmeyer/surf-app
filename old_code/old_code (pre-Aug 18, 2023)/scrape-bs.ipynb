{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e4a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import requests\n",
    "\n",
    "HEADERS = ({'User-Agent':\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \\\n",
    "            (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\\\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.read_csv('breaks.csv', ignore_index)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'break_name': []\n",
    "    , 'break_url': []\n",
    "    , 'time': []\n",
    "    , 'rating': []\n",
    "    , 'period': []\n",
    "    , 'wind': []\n",
    "    , 'wind_state': []\n",
    "    , 'wave_height': []\n",
    "    , 'wave_direction': []\n",
    "    , 'latitude': []\n",
    "    , 'longitude': []\n",
    "})\n",
    "\n",
    "\n",
    "pages_df = pd.read_csv('pages.csv')\n",
    "\n",
    "if len(pages_df) < 3000:\n",
    "    pages_df = pd.DataFrame({'break_name': [], 'url': [], 'page_number': []})\n",
    "\n",
    "    c = True\n",
    "    while c:\n",
    "        print('Page {}'.format(page_num) )\n",
    "        URL = 'https://www.surf-forecast.com/breaks?page={}'.format(5)\n",
    "        webpage = requests.get(URL, headers=HEADERS)\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        breaks = soup.find_all('table', class_='list_table')[0].find_all('a')\n",
    "\n",
    "        if len(breaks) == 0\n",
    "            c = False\n",
    "            continue\n",
    "\n",
    "        for surf_spot in breaks:\n",
    "            pages_df = pd.concat([pages_df, pd.DataFrame({\n",
    "                'break_name': [surf_spot.text]\n",
    "                , 'url': [surf_spot['href']]\n",
    "                , 'page_number': [int(page_num)]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "def scrape(row):\n",
    "    cur_break_df = pd.DataFrame({\n",
    "            'break_name': []\n",
    "            , 'break_url': []\n",
    "            , 'time': []\n",
    "            , 'rating': []\n",
    "            , 'period': []\n",
    "            , 'wind': []\n",
    "            , 'wind_state': []\n",
    "            , 'wave_height': []\n",
    "            , 'latitude': []\n",
    "            , 'longitude': []\n",
    "        })\n",
    "\n",
    "    URL = row['url'] + '/forecasts/latest'\n",
    "\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    dom = etree.HTML(str(soup))\n",
    "\n",
    "\n",
    "    ratings = soup.find_all('tbody')[1].find_all('tr')[2].find_all('td')\n",
    "    times = soup.find_all('tbody')[1].find_all('tr')[1].find_all('td')\n",
    "    periods = soup.find_all('tbody')[1].find_all('tr')[5].find_all('td')    \n",
    "    winds = soup.find_all('tbody')[1].find_all('tr')[8].find_all('td')\n",
    "    wave_heights = soup.find_all('tbody')[1].find_all('tr')[4].find_all('td')\n",
    "    wind_states = soup.find_all('tbody')[1].find_all('tr')[9].find_all('td')\n",
    "    \n",
    "    try:\n",
    "        latitude = soup.find_all('span', class_='latitude')[0]['title']\n",
    "        longitude = soup.find_all('span', class_='longitude')[0]['title']\n",
    "    except:\n",
    "        latitude = None \n",
    "        longitude = None\n",
    "\n",
    "    for time, rating, period, wind, wind_state, wave_height in zip(\n",
    "            times, ratings, periods, winds, wind_states, wave_heights):\n",
    "        cur_break_df = pd.concat([cur_break_df, pd.DataFrame({\n",
    "            'break_name': [row['break_name']]\n",
    "            , 'break_url': [row['url']]\n",
    "            , 'time': [time.text]\n",
    "            , 'rating': [rating.text]\n",
    "            , 'period': [period.text]\n",
    "            , 'wind': [wind.find('text').text]\n",
    "            , 'wind_state': [wind_state.text]\n",
    "            , 'wave_height': [wave_height.find('text').text]\n",
    "            , 'wave_direction': [wave_height.find('text').text]\n",
    "            , 'latitude': [latitude]\n",
    "            , 'longitude': [longitude]\n",
    "        })], ignore_index=True)    \n",
    "\n",
    "    print(cur_break_df)\n",
    "    return cur_break_df\n",
    "\n",
    "for index, row in pages_df.iterrows():\n",
    "    # If this were in production Remove code until HERE and timestamp each pull\n",
    "    if (df['break_name'] == row['break_name']).sum() > 40:\n",
    "        continue\n",
    "    print(row['break_name'])\n",
    "    # HERE\n",
    "\n",
    "    df = pd.concat([df, scrape(row)], ignore_index=True)\n",
    "\n",
    "    print(scrape(row))\n",
    "    data = scrape(row)\n",
    "\n",
    "    csv_name = 'breaks_temp2.csv'\n",
    "    if not os.path.isfile(csv_name):\n",
    "        data.to_csv(csv_name)\n",
    "    else:\n",
    "        data.to_csv(csv_name, mode='a', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2dc55ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AYGA', 'GKA', 'GOROKA', 'GOROKA', 'PAPUA NEW GUINEA', 6, 4, 54, 'S', 145, 23, 30, 'E', 1610, -6.082, 145.392, 1)\n",
      "100 Record inserted successfully into mobile table\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(user=\"avossmeyer\",\n",
    "                                  password=\"surfbro1#\",\n",
    "                                  host=\"surf-forecasts.c6bioghb9ybm.us-east-2.rds.amazonaws.com\",\n",
    "                                  port=\"5432\",\n",
    "                                  database=\"postgres\")\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "#     postgres_insert_query = \"\"\" INSERT INTO mobile (ID, MODEL, PRICE) VALUES (%s,%s,%s)\"\"\"\n",
    "#     record_to_insert = (5, 'One Plus 6', 950)\n",
    "#     cursor.execute(postgres_insert_query, record_to_insert)\n",
    "    cursor.execute(\"\"\" SELECT * FROM airports limit 100\"\"\")\n",
    "    \n",
    "    row = cursor.fetchone()\n",
    "    print(row)\n",
    "\n",
    "\n",
    "    connection.commit()\n",
    "    count = cursor.rowcount\n",
    "    print(count, \"Record inserted successfully into mobile table\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Failed to insert record into mobile table\", error)\n",
    "\n",
    "finally:\n",
    "    # closing database connection.\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"PostgreSQL connection is closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd48272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
